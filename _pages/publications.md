---
layout: archive
title: "Publications and Other Works"
permalink: /publications/
author_profile: true
---

## Peer-reviewed Journals / Conferences
<ol>
    <li value="10">Miyamoto, T., Hashimoto, H., Hayasaka, A., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, and Imaoka, H.
      <br><b>Joint Feature Distribution Alignment Learning for NIR-VIS and VIS-VIS Face Recognition.</b>
      <a href="https://ieeexplore.ieee.org/document/9484385">[IEEE]</a>
      <br><i>International Joint Conference on Biometrics (IJCB) 2021, Virtual conference.</i>
    </li><br>
    <li value="9">Miyagawa, T., and <span style="color:#145094"><b>Ebihara, A.F.</b></span>
      <br><b>The Power of Log-Sum-Exp: Sequential Density Ratio Matrix Estimation for Speed-Accuracy Optimization.</b>
      <a href="https://arxiv.org/abs/2105.13636">[arXiv]</a>
      <br><i>International Conference on Machine Learning (ICML) 2021, Virtual conference.</i>
    </li><br>
    <li value="8"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Sakurai, K., and Imaoka, H. 
      <br><b>Efficient Face Spoofing Detection with Flash.</b>
      <a href="https://ieeexplore.ieee.org/abstract/document/9419913">[IEEE]</a>
      <br><i>IEEE Transactions on Biometrics, Behavior, and Identity Science (T-BIOM) 2021.</i>
    </li><br>
    <li value="7">Imaoka, H., Hashimoto, H., Takahashi, K., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, Liu, J., Hayasaka, A., Morishita, Y., and Sakurai, K.
      <br><b>The Future of Biometrics Technology: from Face Recognition to Related Applications.</b>
      <br><i>APSIPA Transactions on Signal and Information Processing, 2021.</i>
    </li><br>
    <li value="6"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Miyagawa, T., Sakurai, K., and Imaoka, H. 
      <br><b>Sequential Density Ratio Estimation for Simultaneous Optimization of Speed and Accuracy.</b>
      <a href="https://arxiv.org/abs/2006.05587">[arXiv]</a>
      <br><i>International Conference on Learning Representations (ICLR) 2021, Virtual conference.</i>
      <br><b><a href="https://openreview.net/forum?id=Rhsu5qD36cL&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2021%2FConference%2FAuthors%23your-submissions)">*Spotlight presentation (top 5.57%).</a></b>
    </li><br>
    <li value="5"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Sakurai, K., and Imaoka, H.  
      <br><b>Specular- and Diffuse-reflection-based Face Spoofing Detection for Mobile Devices.</b>
      <a href="https://arxiv.org/abs/1907.12400">[arXiv]</a>
      <a href="https://ieee-biometrics.org/ijcb2020/Program.html#apaper">[IJCB2020]</a>
      <br><i>International Joint Conference on Biometrics (IJCB) 2020, Virtual conference.</i>
      <br><b><a href="https://ieee-biometrics.org/ijcb2020/Program.html#awards">*IJCB 2020 Google Best Paper Award</a></b>
    </li><br>
    <li value="4">Caruso, V.C., Mohl, J.T., Glynn, C., Jee, J., Willets, S.M., Azman, A., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, Estrada, R., Freiwald, W.A., Tokdar, S.T., and Groh, J.M. 
      <br><b>Single Neurons May Encode Simultaneous Stimuli by Switching Between Activity Patterns.</b>
      <a href="https://www.nature.com/articles/s41467-018-05121-8">[NatComm]</a>
      <br><i>Nature Communications 9, 2715 (2018).</i>
      </li><br>
    <li value="3"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Hocevar, A., Magnasco M.O., and Freiwald, W.A. 
      <br><b>Multi-object Scene Encoding in Face-selective Cortex.</b>
      <br><i>Manuscript in preparation.</i>
    </li><br>
    <li value="2">Leibo, J.Z., Anselmi, F., Mutch J., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, Freiwald, W.A., and Poggio, T. 
      <br><b>View-invariance and Mirror-symmetric Tuning in a Model of the Macaque Face-processing System. </b>
      <br><i>Cosyne 2013, Salt Lake City, UT,  USA.</i>
    </li><br>
    <li value="1"><span style="color:#145094"><b>Ebihara, A.F.</b></span> and Freiwald, W.A. 
      <br><b>Representation of Multiple Stimuli in the Macaque Middle Face Patch. </b>
      <br><i>Cosyne 2012, Salt Lake City, UT, USA.</i>
    </li><br>
</ol>

## Non-peer-reviewed Works  
<ol>
    <li value="15">Miyamoto, T., Hashimoto, H., Hayasaka, A., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, and Imaoka, H. 
      <br><b>Joint Feature Distribution Alignment Learning for NIR-VIS and VIS-VIS Face Recognition.</b>
      <br><i>The 11th Symposium on Biometrics, Recognition and Authentication (IEICE SBRA2021), Virtual Conference.</i>
    </li><br>
    <li value="14">Kawai, H., Watanabe, D., Ito, K., Aoki, T., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, Sakurai, K., and Imaoka, H. 
      <br><b>Spoofing Detection for the Face Authentication Using Deep Learning.</b>
      <br><i>2019 Symposium on Cryptography and Information Security. (IEICE SCIS) Otsu city, Shiga, Japan.</i>
    </li><br>
    <li value="13"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Sakurai, K., and Imaoka, H. 
      <br><b>Flash Reflection-based Face Liveness Detection for Mobile Devices.</b>
      <br><i>The 8th Symposium on Biometrics, Recognition and Authentication (IEICE SBRA2018), Iidabashi, Tokyo, Japan.</i>
    </li><br>
    <li value="12"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Sakurai, K., and Imaoka, H. 
      <br><b>Spoofing Detection with a Monocular Camera using Flash Reflection.</b>
      <br><i>IEICE Bio-X Study Group 2018. Kanazawa city, Ishikawa, Japan.</i>
    </li><br>
    <li value="11">Caruso, V.C., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, Tokdar, S., Freiwald, W.A., and Groh, J.M. 
      <br><b>Multiplexing in Face Selective Cortex: Evidence of Flexible Trial-by-Trial and Sub-Trial Representations of Multiple Stimuli</b>
      <br><i>Conference on Cognitive Computational Neuroscience (CCN)	2017. Columbia University, NY.</i>
    </li><br>
    <li value="10">Caruso, V.C., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, Milewski A., Tokdar, S., Freiwald W.A., and Groh J.M. 
      <br><b>Is Multiplexing a General Strategy for Encoding Multiple Items in the Brain? Evidence From a Visual Cortical Face Area and a Subcortical Auditory Area.</b>
      <br><i>Society for Neuroscience annual meeting 2015, Poster session on Oscillations and synchrony: Unit studies. Chicago, IL.</i>
      </li><br>
    <li value="9"><span style="color:#145094"><b>Ebihara, A.F.</b></span>   
      <br><b>A Circuit Mechanism for Explicit Multi-stimulus Coding.</b>
      <br><i>JMSA New York Life Science Forum 2015, New York, NY.</i>
    </li><br>
    <li value="8"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Hocevar, A., Magnasco M.O., and Freiwald, W.A.
      <br><b>Representation of Multiple Stimuli by Face Selective Neurons in the Macaque Temporal Lobe.</b>
      <br><i>JMSA New York Life Science Forum 2015, New York, NY.</i>
    </li><br>
    <li value="7"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Hocevar, A., Magnasco M.O., and Freiwald, W.A. 
      <br><b>Representations of Multiple Stimuli by Face-selective Neurons in the Macaque Temporal Lobe.</b>
      <br><i>Society for Neuroscience annual meeting 2014, Poster session on Neural Processing of Faces and Bodies, Washington, DC.</i>
    </li><br>
    <li value="6"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Hocevar, A., Magnasco M.O., and Freiwald, W.A. 
      <br><b>Representations of Multiple Stimuli in the Macaque Temporal Lobe.</b>
      <br><i>Cold Spring Harbor Symposium on Quantitative Biology 2014: Cognition, Cold Spring Harbor, NY.</i>
    </li><br>
    <li value="5"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Leibo, J.Z., Poggio, T., and Freiwald, W.A. 
      <br><b>Max-pooling and Clutter Tolerance in the Macaque Face Processing System.</b>
      <br><i>NSF-STC site visit at McGovern Institute for Brain Research (MIT), 2012, Cambridge, MA.</i>
    </li><br>
    <li value="4"><span style="color:#145094"><b>Ebihara, A.F.</b></span> and Freiwald, W.A.   
      <br><b>Representations of Multiple Stimuli in the Macaque Face Patches.</b>
      <br><i>Society for Neuroscience annual meeting 2012, Poster session on Neural Processing of Faces and Bodies, Session 263.10/Y19, New Orleans, LA.</i>
    </li><br>
    <li value="3"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Tsao, D.Y., and Freiwald, W.A.
      <br><b>Representation of Multiple Stimuli by Face-selective Neurons in the Macaque Temporal Lobe.</b>
      <br><i>Society for Neuroscience annual meeting 2011, Poster session on Face Processing in Extrastriate Cortex,Session 487.06/OO36, Washington, DC.</i>
      </li><br>
    <li value="2"><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Tsao, D.Y., and Freiwald, W.A.
      <br><b>Representations of Multiple Objects in the Macaque Face Processing System.</b>
      <br><i>Society for Neuroscience annual meeting 2010, Nanosymposium on Encoding of Visually Presented Faces II, Session 834.4, San Diego, CA.</i>
    </li><br>
    <li value="1">Nishizumi, H., Nakayama, H, <span style="color:#145094"><b>Ebihara, A.F.</b></span>, and Sakano, H.
      <br><b>Odorant Receptor Gene Choice in the Mouse Olfactory System.</b>
      <br><i>The 31st Annual Meeting of the Japan Neuroscience Society, 2008, Oral session O1-H04, Tokyo, Japan.</i>
    </li><br>
</ol>

## Ph.D. Thesis
<p style='margin-left:30.0pt'>
<span style="color:#145094"><b>Ebihara, A.F.</b></span>
<br><b>Normalization Among Heterogeneous Population Confers Stimulus Discriminability on the Macaque Face Patch Neurons.</b>
<br><i>2015 Student Theses and Dissertations. 278.</i> 
<a href="https://digitalcommons.rockefeller.edu/student_theses_and_dissertations/278">[link]</a>
<br><br>
Ph.D. advisor: Dr. Winrich A. Freiwald<br>
Ph.D. co-advisor: Dr. Marcelo Magnasco<br>
Faculty advisory committee: Dr. Cornelia I. Bargmann and Dr. Charles D. Gilbert<br>
<br></p>

## Patents  
<ol>
    <li value="13"><span style="color:#145094"><b>Ebihara, A.</b></span>, and Miyagawa, T.
      <br><b>Learning device, learning method, and recording medium.</b>
      <br>WO2021229663A1
      <!-- <br><i>published on Nov. 18th, 2021.</i> -->
    </li><br>
    <li value="12"><span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Determination device, determination method, and recording medium.</b>
      <br>WO2021229662A1
      <!-- <br><i>published on Nov. 18th, 2021.</i> -->
    </li><br>
    <li value="11"><span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Determination device, determination method, and recording medium.</b>
      <br>WO2021229661A1
      <!-- <br><i>published on Nov. 18th, 2021.</i> -->
    </li><br>
    <li value="10"><span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Determination device, learning method, and recording medium.</b>
      <br>WO2021229660A1
      <!-- <br><i>published on Nov. 18th, 2021.</i> -->
    </li><br>
    <li value="9"><span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Determination device, determination method, and recording medium.</b>
      <br>WO2021229654A1
      <!-- <br><i>published on Nov. 18th, 2021.</i> -->
    </li><br>
    <li value="8">Miyagawa, T., and <span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Identification device, identification method, and recording medium.</b>
      <br>WO2021220450A1
      <!-- <br><i>published on Nov. 4th, 2021.</i> -->
    </li><br>
    <li value="7"><span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Weight estimation device, weight estimation method, and weight estimation program.</b>
      <br>JP2021021973A
      <!-- <br><i>published on Feb. 18th, 2021.</i> -->
    </li><br>
    <li value="6"><span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Spoofing detection device, spoofing detection method, and program.</b>
      <br>JPWO2019163066A1
      <!-- <br><i>published on Feb. 18th, 2021.</i> -->
    </li><br>
    <li value="5"><span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Spoofing detection device, spoofing detection method, and program.</b>
      <br>JPWO2019163065A1
      <!-- <br><i>published on Jan. 7th, 2021.</i> -->
    </li><br>
    <li value="4"><span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Spoofing detection apparatus, spoofing detection method, and computer-readable recording medium.</b>
      <br>JPWO2019163066A1, US20210034894A1, WO2019163066A1
      <!-- <br><i>published on Feb. 4th, 2021.</i> -->
    </li><br>
    <li value="3"><span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Spoofing detector, spoofing detection method, and computer-readable recording medium.</b>
      <br>JPWO2019163065A1, EP3757936A1, US20210034893A1, WO2019163065A1
      <!-- <br><i>published on Dec. 30th, 2020.</i> -->
    </li><br>
    <li value="2"><span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Information processing device, personal identification device, information processing method, and storage medium.</b>
      <br>WO2020194497A1
      <!-- <br><i>published on Oct. 1st, 2020.</i> -->
      <br><i><b>*Covers the idea of the SPRT-TANDEM.</b></i>
    </li><br>
    <li value="1"><span style="color:#145094"><b>Ebihara, A.</b></span>
      <br><b>Face three-dimensional shape estimation device, face three-dimensional shape estimation method, and face three-dimensional shape estimation program.</b>
      <br>JPWO2019078310A1,EP3699865A4, US20200257888A1, WO2019078310A1
      <!-- <br><i>published on Oct. 1st, 2020.</i> -->
    </li><br>

</ol>

<!-- ## Patents  
<table>
  <tr>
    <td>   </td>
    <td><b>Patent number</b></td>
    <td><b>Authors and applied year</b></td>
    <td><b>Description</b></td>
    <td><b>Status</b></td>
  </tr>
  <tr>
    <td>[31]</td>
    <td>N.A.</td>
    <td>Watanabe, K., Miyagawa, T., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2021. </td>
    <td>Undisclosed (SPRT-related)</td>
    <td>in prep.</td>
  </tr>
  <tr>
    <td>[30]</td>
    <td>N.A.</td>
    <td>Watanabe, K., Miyagawa, T., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2021. </td>
    <td>Undisclosed (SPRT-related)</td>
    <td>in prep.</td>
  </tr>
  <tr>
    <td>[29]</td>
    <td>N.A.</td>
    <td>Miyagawa, T., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2021. </td>
    <td>Undisclosed (SPRT-related)</td>
    <td>in prep.</td>
  </tr>
  <tr>
    <td>[28]</td>
    <td>N.A.</td>
    <td>Miyagawa, T., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2021. </td>
    <td>Undisclosed (SPRT-related)</td>
    <td>in prep.</td>
  </tr>
  <tr>
    <td>[27]</td>
    <td>N.A.</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Miyagawa, T., 2021.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>in prep.</td>
  </tr>
  <tr>
    <td>[26]</td>
    <td>N.A.</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Miyagawa, T., 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>in prep.</td>
  </tr>
  <tr>
    <td>[25]</td>
    <td>N.A.</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Miyagawa, T., 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>in prep.</td>
  </tr>
  <tr>
    <td>[24]</td>
    <td>N.A.</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Miyagawa, T., 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>in prep.</td>
  </tr>
  <tr>
    <td>[23]</td>
    <td>JP2020/034545</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, Miyagawa, T., 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[22]</td>
    <td>JP2020/033474</td>
    <td>Miyagawa, T., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[21]</td>
    <td>JP2020/018236</td>
    <td>Miyagawa, T., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[20]</td>
    <td>JP2020/018889</td>
    <td>Miyagawa, T., <span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[19]</td>
    <td>JP2020/018888</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[18]</td>
    <td>JP2020/018887</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[17]</td>
    <td>JP2020/018886</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[16]</td>
    <td>JP2020/018883</td>
    <td>Ka Pik, L.,<span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[15]</td>
    <td>JP2020/018884</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Undisclosed (SPRT-related)</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[14]</td>
    <td>特願2019-136145</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2019.</td>
    <td>Luggage weight detection from a video</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[13]</td>
    <td>JP2019/012793</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2019.</td>
    <td>Binary- and multi-class-Nth-order SPRT</td>
    <td>valid</td>
  </tr>
  <tr>
    <td>[12]</td>
    <td>US16/971460</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Face spoofing detection with diffusion reflection</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[11]</td>
    <td>特願2020-501933</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Face spoofing detection with diffusion reflection</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[10]</td>
    <td>JP2018/006562</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2018.</td>
    <td>Face spoofing detection with diffusion reflection</td>
    <td>expired</td>
  </tr>
  <tr>
    <td>[9]</td>
    <td>EP18907036.0</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Face spoofing detection with specular reflection</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[8]</td>
    <td>US16/964877</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Face spoofing detection with specular reflection</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[7]</td>
    <td>特願2020-501932</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Face spoofing detection with specular reflection</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[6]</td>
    <td>JP2018/006561</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2018.</td>
    <td>Face spoofing detection with specular reflection</td>
    <td>expired</td>
  </tr>
  <tr>
    <td>[5]</td>
    <td>EP18869108.3</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Face 3D-ness detection on a mobile device</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[4]</td>
    <td>US16/755983</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2020.</td>
    <td>Face 3D-ness detection on a mobile device</td>
    <td>pending</td>
  </tr>
  <tr>
    <td>[3]</td>
    <td>特願2019-549345</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2018.</td>
    <td>Face 3D-ness detection on a mobile device</td>
    <td>valid</td>
  </tr>
  <tr>
    <td>[2]</td>
    <td>JP2018/038898</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2018.</td>
    <td>Face 3D-ness detection on a mobile device</td>
    <td>expired</td>
  </tr>
  <tr>
    <td>[1]</td>
    <td>特願2017-203669</td>
    <td><span style="color:#145094"><b>Ebihara, A.F.</b></span>, 2017.</td>
    <td>Face 3D-ness detection on a mobile device</td>
    <td>expired</td>
  </tr>
</table>   -->


## Media
Nov 11th, 2021. <b>LabBase.jp</b>  
<a href="https://compass.labbase.jp/articles/799">異分野キャリアって有り？　研究力重視のNEC（日本電気株式会社）で、神経科学者が「顔認証」で活躍する理由</a>  

May 7th, 2021. <b>NEC official press release</b>  
<a href="https://jpn.nec.com/press/202105/20210506_03.html">NEC、複雑な意思決定を行う際の脳活動の知見を応用したAI技術を開発  
～「早押しクイズ」にヒントを得て、顔認証やサイバー攻撃の分析を最大20倍高速化～</a>  

May. 7th, 2021. <b>Nikkei (日本経済新聞, 朝刊, 10面, 530文字)</b>  
NEC「一瞬」で顔認証、最短0.1秒未満、従来の20分の1、精度保ち、利便性向上  

May. 7th, 2021. <b>Cloud Watch (クラウドwatch)</b>  
<a href="https://cloud.watch.impress.co.jp/docs/news/1322762.html">NEC、時系列データのリアルタイム分析向け新技術を開発　高い精度を維持しながら高速な判断を可能に</a>  

May. 7th, 2021. <b>EnterpriseZine</b>  
<a href="https://enterprisezine.jp/news/detail/14351">NEC、「早押しクイズ」をヒントにAI技術を開発</a>  

May. 7th, 2021. <b>Robotstart (ロボスタ)</b>  
<a href="https://robotstart.info/2021/05/06/nec-ai-technology-sprt-tandem.html">簡単な問題は素早く回答、難問はじっくり考えてから答えを出すAIをNECが開発　顔認証の処理を最大20倍高速化　脳科学にヒント</a>  

May. 7th, 2021. <b>ASCII×AI</b>  
<a href="https://ascii.jp/elem/000/004/053/4053836/">NEC、高い精度を維持しながら高速で判断するAI技術を開発</a>  

May. 7th, 2021. <b>Yahoo News</b>  
<a href="https://news.yahoo.co.jp/articles/8485361becd0ec9df6d751993606860e8125b4ba">NEC、高い精度を維持しながら高速で判断するAI技術を開発</a>  

May. 6th, 2021. <b>Nikkei Online (日経電子版)</b>  
<a href="https://www.nikkei.com/article/DGXZQOUC211IQ0R20C21A4000000/  ">NEC、顔認証の判定時間短縮　最大で20分の1に</a>  

Feb. 17th, 2017.　<b>President Online</b>  
<a href="https://president.jp/articles/-/21367?page=2">高学歴ワーキングプア「ポスドク問題」は解決できたか？</a>  



<!-- {% if author.googlescholar %}
  You can also find my articles on <u><a href="{{author.googlescholar}}">my Google Scholar profile</a>.</u>
{% endif %}

{% include base_path %}

{% for post in site.publications reversed %}
  {% include archive-single.html %}
{% endfor %} -->
